{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Peyton Smith Computer Engineer"},{"location":"#objective","title":"Objective","text":"<p>As a skilled and driven computer engineer, my objective is to leverage my expertise in software development, systems architecture, and problem-solving to contribute to innovative technology solutions. With a strong foundation in Python, C++, hardware design and a passion for cutting-edge advancements, I aim to collaborate with passionate teams to design and develop advanced technologies in AI, robotics, and space. My goal is to apply my passion, effective cooperation, and continuous learning to create impactful and meaningful work in these industries.</p>"},{"location":"#education","title":"Education","text":"<p>I will be graduating with a Bachelor of Science in Computer Engineering from Texas A&amp;M University in May of 2024. I have a strong foundation in computer engineering concepts, gained through coursework and hands-on projects. While attending Texas A&amp;M University I had the opportunity to study abroad for a semester at Singapore Technology and Design. There, I had the opportunity to study Artificial Intelligence and Machine Learning under a MIT designed curriculum. I collaborated with peers from all parts of the world to complete challenging projects in these subjects. My education has equipped me with a comprehensive skill set, combining theoretical knowledge with practical application, positioning me for success in the field of computer engineering.</p>"},{"location":"#skills","title":"Skills","text":""},{"location":"#technical","title":"Technical","text":"<p><code>Oscilloscope</code> <code>Verilog</code> <code>C</code> <code>C++</code> <code>C#</code> <code>Python</code> <code>SwiftUI</code> <code>Git</code> <code>Scaled Agile Framework</code></p>"},{"location":"#soft","title":"Soft","text":"<p><code>Leadership</code> <code>Active Listening</code> <code>Emotional Intelligence</code> <code>Problem Solving</code> <code>Collaboration</code> <code>Communication</code> <code>Time Management</code> </p>"},{"location":"#interests","title":"Interests","text":"<p>In my personal life, I have a few hobbies that bring me satisfaction and relaxation. Rock climbing is one of my favorite things to do after work and on the weekends. It is an invigorating activity that allows me to challenge myself physically and immerse in nature. Photography is another pastime that challenges me creatively and is my favorite way to enjoy a new place when traveling. Additionally, I always enjoy a good movie. You will find me keeping up with the most recent content from noteworthy directors and franchises. These hobbies add a sense of fulfillment and variety to my everyday life, making it enjoyable through exploration and self-expression.</p>"},{"location":"#volunteerism","title":"Volunteerism","text":"<p>Volunteer work and charity is an important value of mine. There are many ways to perform volunteer work, all of which are crucial. Mental health is the avenue that I find most fulfilling and impactful, being that I see mental health challenges affect friends, family, and even myself. It is something everyone will encounter one way or another in this life. Below are experiences, current and past, that I support.</p>"},{"location":"#texas-am-helpline","title":"Texas A&amp;M HelpLine","text":"<p>HelpLine exists to be an after-hours mental health service for Texas A&amp;M University. They seek to provide a free confidential, non-judgemental, safe place for students to go to for any mental health related topics. They provide peer support, information, crisis intervention, and referrals to students. As a volunteer, I aided my peers by providing them a listening ear through this service. It has brought immense satisfaction to my life being able to support people when they do not feel they have that in their lives.</p>"},{"location":"#the-church-of-jesus-christ-of-latter-day-saints","title":"The Church of Jesus Christ of Latter-Day Saints","text":"<p>Another important facet of my life is The Church of Jesus Christ of Latter-Day Saints. Being a member, I had the privilege to serve as a full-time volunteer Missionary in Brazil, Cuiaba. From August 2018 to March 2020 I was able to serve, teach, and lead in this area of the world about Christ. We served the local community by organizing and completing service projects, getting to know local members, and serving in volunteer positions at the local church. There I had the opportunity to meet some of the most amazing people. They were always willing to serve and help us too when we needed help. It was an amazing experience that taught me so much compassion, perspective, and humility.</p>"},{"location":"experience/aggie_sat_lab/","title":"AggieSat Laboratory","text":""},{"location":"experience/aggie_sat_lab/#cdh-team-leader-august-2021-february-2022","title":"CDH Team Leader, August 2021 - February 2022","text":""},{"location":"experience/aggie_sat_lab/#about-the-organization","title":"About the Organization","text":"<p>AggieSat Laboratory is a student-run space program that seeks to give students hands-on experience in working with space systems. They have put two satellites into orbit since being created and competed for over 10 grants. They currently have 5 active projects in development.</p>"},{"location":"experience/aggie_sat_lab/#role","title":"Role","text":"<p>NASA sent out a request for proposals for a $2 million grant for an autonomous lunar rover. I was apart of Aggie Sat's ALARM project. This was an Autonomous Lunar Regolith Manipulator. Its purpose was to be deployed to prepare lunar regolith for a rocket to land on. It is essentially an autonomous lunar bull dozer. </p> <p>Aggie Sat had submitted a proposal and I was on the Command and Data Handling (CDH) team for this project. We performed preliminary design work on the rover by creating FDIR documents that conceptualized the requirements for the CDH sub-system on board the rover. During this time I was promoted to CDH Team Lead and managed the sub team in completing weekly tasks. </p>"},{"location":"experience/aggie_sat_lab/#skills-acquiredutilized","title":"Skills Acquired/Utilized","text":""},{"location":"experience/aggie_sat_lab/#technical","title":"Technical","text":"<p><code>Proposal Planning</code> <code>Fault Detection and Isolation concepts</code></p>"},{"location":"experience/aggie_sat_lab/#soft","title":"Soft","text":"<p><code>Leadership</code></p>"},{"location":"experience/akw_ventures/","title":"AKW Ventures LLC","text":""},{"location":"experience/akw_ventures/#software-developer-august-2021-current","title":"Software Developer, August 2021 - Current","text":""},{"location":"experience/akw_ventures/#about-the-company","title":"About the Company","text":"<p>AKW Ventures LLC, SAAS company run by Chris Wilke, seeks to provide quality software in areas needed. One of its core products is the College Footballer App. This app provides the times, channels, rosters, and live score for customers. The user is able to customize and follow their favorite teams and have their stats easily available. </p>"},{"location":"experience/akw_ventures/#role","title":"Role","text":"<p>As a developer on the team for College Footballer, I worked on a small team to update and improve the existing code base of the College Football schedules app suite. I assisted in creating recurring revenue by implementing a subscription popup with Revenue Cat and SwiftUI. </p> <p>Additionally, I helped automate data processing to retrieve live information for users. By utilizing cron on Unix I was able to automate schedules tasks or scripts to be run at needed times in order to update live score information. </p>"},{"location":"experience/akw_ventures/#skills-acquiredutilized","title":"Skills Acquired/Utilized","text":""},{"location":"experience/akw_ventures/#technical","title":"Technical","text":"<p><code>Swift/SwiftUI</code> <code>Agile Development Process</code> <code>Git</code> <code>Xcode</code> <code>Cron</code> <code>Unix</code></p>"},{"location":"experience/lm/","title":"Lockheed Martin Aeronautics","text":""},{"location":"experience/lm/#sensor-fusion-software-engineering-intern-may-2023-august-2023","title":"Sensor Fusion Software Engineering Intern, May 2023 - August 2023","text":""},{"location":"experience/lm/#about-the-company","title":"About the Company","text":"<p>Lockheed Martin is a global aerospace and defense company. The support a wide range of advanced military aircraft and related technologies. Lockheed Martin Aeronautics is one of four major business segments of Lockheed Martin Corp that is best known for is creation and production of the F-35 Lightning II, F-22, and F-16 fighter jets.</p>"},{"location":"experience/lm/#role","title":"Role","text":"<p>As an intern I worked on the Sensor Fusion team for airframes. While working as an intern I was trained in the fundamental concepts of sensor fusion such as the Kalman Filter, Global Nearest Neighbor, and frames of reference. These algorithms are fundamental to data association and sensor tracking. </p> <p>Additionally I supported the team by developing an application to analyze a software libraries architecture based off of principles of Clean Architecture by Bob Martin. By creating this application, I developed a solid understanding of the principles behind good architecture design. These principles include circular dependencies, instability, abstractness, and encapsulation. </p> <p>While developing this tool I worked effectively in a team of six. We utilized the Agile development framework to complete tasks according to the product owners needs, planed sprints, demonstrated complete products, and evaluated teams effectiveness. </p>"},{"location":"experience/lm/#skills-acquiredutilized","title":"Skills Acquired/Utilized","text":""},{"location":"experience/lm/#technical","title":"Technical","text":"<p><code>Sensor Fusion</code> <code>Data Association</code> <code>C#</code> <code>ASP.NET</code> <code>Winforms</code> <code>CMake</code> <code>Python</code> <code>Git</code> <code>Scaled Agile Development</code></p>"},{"location":"experience/sec_directed_internship/","title":"Student Engineers' Council","text":""},{"location":"experience/sec_directed_internship/#directed-intern-may-2022-august-2022","title":"Directed Intern, May 2022 - August 2022","text":""},{"location":"experience/sec_directed_internship/#about-the-organization","title":"About the Organization","text":"<p>The Student Engineers' Council exists solely to provide advancement to students in the College of Engineering at Texas A&amp;M University. It's mission is to be the representative voice of all engineering students, increase engineering awareness, and foster professional advancement of all engineering students at Texas A&amp;M University. To accomplish this mission they put on programs such as the yearly engineering career fair and the directed internship. </p>"},{"location":"experience/sec_directed_internship/#role","title":"Role","text":"<p>As a direct intern I was placed on a team of five and was tasked to research and devise a product solution for satellite collision avoidance with space debris and other orbital objects. As a team, we researched about the growing need to solve this problem. As more and more satellites enter low and high earth orbit, it become increasingly more likely that a collision will occur not just with other satellites but with the hundreds of thousands of space debris tracked and untracked. </p> <p>Through out the course of the summer my team and I worked in MATLAB to propagate orbital paths of know space debris tracks put out the Department of Defense. We utilized the SGP4 and SDP4 orbital propagators to accomplish this task. You can view this in our GitHub. After that we worked to design a business model, financial plan, and product to present to a board at the end of the internship. </p>"},{"location":"experience/sec_directed_internship/#skills-acquiredutilized","title":"Skills Acquired/Utilized","text":""},{"location":"experience/sec_directed_internship/#technical","title":"Technical","text":"<p><code>MATLAB</code> <code>Orbit Propagation, SGP4 and SDP4</code> <code>Git</code> <code>Business Plan Development</code></p>"},{"location":"experience/sec_directed_internship/#soft","title":"Soft","text":"<p><code>Collaboration</code> <code>Presentation</code></p>"},{"location":"experience/template/","title":"Company Name","text":""},{"location":"experience/template/#your-roll-date","title":"Your Roll, date","text":""},{"location":"experience/template/#about-the-company","title":"About the Company","text":"<p>The Company is...</p>"},{"location":"experience/template/#role","title":"Role","text":"<p>My role was...</p>"},{"location":"experience/template/#skills-acquiredutilized","title":"Skills Acquired/Utilized","text":"<ul> <li>Skill</li> </ul>"},{"location":"notes/kalman_filter_1d/","title":"Kalman Filter","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\nLEN = 200\n\n# Simulate random measured data of a constant value\nRAN = np.array([i for i in np.linspace( 20, 25, num = 256)])\n\n# Simulate random variances, 1-2.58 because small variances cause underflow issues\nCOV = np.array([i for i in np.linspace(1, 2.58, 256)])\n\nTIME = np.arange(0, 100, .5)\n\ndef setData():\n    x = np.array([])\n    for i in range(LEN):\n        x = np.append(x, random.choice(RAN))\n    return x\n\ndef setCovariance():\n    x = np.array([])\n    for i in range(LEN):\n        x = np.append(x, random.choice(COV))\n    return x\n\nx = setData()\np = setCovariance()\n</pre> import numpy as np import random import matplotlib.pyplot as plt  LEN = 200  # Simulate random measured data of a constant value RAN = np.array([i for i in np.linspace( 20, 25, num = 256)])  # Simulate random variances, 1-2.58 because small variances cause underflow issues COV = np.array([i for i in np.linspace(1, 2.58, 256)])  TIME = np.arange(0, 100, .5)  def setData():     x = np.array([])     for i in range(LEN):         x = np.append(x, random.choice(RAN))     return x  def setCovariance():     x = np.array([])     for i in range(LEN):         x = np.append(x, random.choice(COV))     return x  x = setData() p = setCovariance() In\u00a0[2]: Copied! <pre># Time loop - each iteration represents a time stamp with data measured and collected\ne_est = p[0]\nest_t = x[0]\n\ne = np.array([est_t])\n\nfor i in range (1, LEN, 1):\n    # Calculate Kalman gain e_est / (e_est + e_mear)\n    kg = e_est / (e_est + p[i])\n\n    # Get new estimate: est_t = est_t-1 + KG[mea - est_t-1]\n    est_t = est_t + (kg * (x[i] - est_t))\n\n    # Calculate new error with est_t: E_est_t = [1-KG](e_est_t-1)\n    e_est = (1-kg) * (e_est) \n\n    # Store\n    e = np.append(e, est_t)\n\nplt.scatter(TIME,x)\nplt.scatter(TIME,e)\nplt.ylim((15,25))\nplt.show()\n</pre> # Time loop - each iteration represents a time stamp with data measured and collected e_est = p[0] est_t = x[0]  e = np.array([est_t])  for i in range (1, LEN, 1):     # Calculate Kalman gain e_est / (e_est + e_mear)     kg = e_est / (e_est + p[i])      # Get new estimate: est_t = est_t-1 + KG[mea - est_t-1]     est_t = est_t + (kg * (x[i] - est_t))      # Calculate new error with est_t: E_est_t = [1-KG](e_est_t-1)     e_est = (1-kg) * (e_est)       # Store     e = np.append(e, est_t)  plt.scatter(TIME,x) plt.scatter(TIME,e) plt.ylim((15,25)) plt.show()"},{"location":"notes/liquid_nueral_networks/","title":"Liquid nueral networks","text":""},{"location":"notes/liquid_nueral_networks/#attention","title":"Attention","text":""},{"location":"notes/liquid_nueral_networks/#resiliency-maps","title":"Resiliency Maps","text":""},{"location":"notes/liquid_nueral_networks/#kaggle-public-data-sets-hugging-face-data-sets","title":"Kaggle public data sets, hugging face data sets","text":""},{"location":"notes/mkdocs_github_actions/","title":"Peyton's tutorial for Mkdocs and GitHub Pages","text":""},{"location":"notes/mkdocs_github_actions/#step-1-setting-up-the-repository","title":"Step 1 - Setting up the repository","text":"<ul> <li>Create your repository.</li> <li>Add mkdocs.yml and docs directory with the index.md page.</li> <li>Commit all changes to main.</li> </ul>"},{"location":"notes/mkdocs_github_actions/#step-2-using-github-actions","title":"Step 2 - Using GitHub Actions","text":"<ul> <li>Create a new directory titled .github/workflows within the repository.</li> <li>Create a .yml file in the workflows directory.</li> <li>Insert the following lines into the .yml file.</li> </ul> <pre><code>name: ci \non:\n  push:\n    branches:\n      - master \n      - main\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n        with:\n          python-version: 3.x\n      - run: pip install mkdocs-material \n      - run: mkdocs gh-deploy --force\n</code></pre> <ul> <li>The above code creates a GitHub action that executes everytime there is a push to the repository. In this case it is installing all the prerequisites and executing the command <code>mkdocs gh-deploy --force</code> which deploys the mkdocs again on GitHub Pages.</li> <li>Push this directory and file to the main branch and an action is created and will run on push.<ul> <li>Note: I experienced up to 5-7 minute wait times for the update to occur on the page.</li> </ul> </li> <li>Added a new line.</li> </ul>"},{"location":"notes/mkdocs_github_actions/#step-3-include-jupyter-notebooks-and-py-files","title":"Step 3 - Include Jupyter Notebooks and .py Files","text":"<ul> <li>Add the below to teh <code>mkdocs.yml</code> file. This can be edited for includes and ignores for each notebook.</li> </ul> <pre><code>plugins:\n  - mkdocs-jupyter:\n      include: [\"*.py\", \"*.ipynb\"]\n      ignore: [\"some-irrelevant-files/*.ipynb\"]\n      execute: true\n      execute_ignore:\n        - \"my-secret-files/*.ipynb\"\n      allow_errors: false\n      kernel_name: python3\n      theme: dark\n</code></pre> <ul> <li>Add the path to the .ipynb or .py file as a regular page like you would an .md file and it will render.</li> <li>Make changes the mkdocs and commit them to main and vualah! </li> </ul>"},{"location":"notes/mkdocs_github_actions/#issues-i-came-across","title":"Issues I came across","text":"<ul> <li>Error message in GitHub Desktop when trying to commit changes: <code>error: couldn't set 'refs/remotes/origin/main' From https://github.com/Peyton-Smith05/my-page  ! 35ec70b..c5d2301  main       -&gt; origin/main  (unable to update local ref)</code><ul> <li>Resolution: Running this command <code>git gc --prune=now</code> on linux command line in workflows directory<ul> <li>(https://stackoverflow.com/questions/10068640/git-error-on-git-pull-unable-to-update-local-ref)[Source] </li> </ul> </li> </ul> </li> </ul>"},{"location":"notes/mkdocs_github_actions/#sources","title":"Sources","text":"<ul> <li>(https://squidfunk.github.io/mkdocs-material/publishing-your-site/)[Github actions code]<ul> <li>(https://docs.github.com/en/actions/quickstart)[More] on GitHub Actions</li> </ul> </li> <li>(https://www.mkdocs.org/user-guide/deploying-your-docs/#custom-domains)[Deploying mkdocs on GitHub Pages]<ul> <li>Note: There is a second way mentioned in this site for User Pages</li> </ul> </li> <li>(https://squidfunk.github.io/mkdocs-material/reference/lists/#using-definition-lists)[Mkdocs styling and formatting]</li> <li>(https://pages.github.com/)[Creating a GitHub Page]</li> <li>(https://github.com/danielfrg/mkdocs-jupyter)[Adding Jupyter Notebooks]</li> </ul>"},{"location":"notes/revenue_cat/","title":"Revenue Cat SetUp Documentation","text":"<ul> <li>See here for more information</li> </ul>"},{"location":"notes/revenue_cat/#definitions","title":"Definitions","text":"<ul> <li> <p>Revenue Cat Product: Individual SKUs (Stock Keeping Unit) that users actually purchase. The stores (Apple, Google, Stripe, and Amazon) process these SKUs and charge the user.</p> <ul> <li> <p>Rapid Roster Product: Each individual app and their associated subscription purchase.</p> </li> <li> <p>This is linked to Revenue Cat with a shared secret and Bundle ID</p> </li> </ul> </li> <li> <p>Revenue Cat Entitlement: An entitlement represents a level of access, features, or content that a user is \"entitled\" to.</p> <ul> <li> <p>This is going to contain links to products. For example and app may have a \"Gold\" access but there are two different payment plans of monthly and yearly within App Store Connect, but they both unloack the same level of access. So both products are within the same entitlement.</p> </li> <li> <p>Rapid Roster Entitlement: \"Subscribed\".</p> </li> </ul> </li> <li> <p>Revenue Cat Package: a group of equivalent products across iOS, Android, and web.</p> </li> <li> <p>Offerings: The selection of products that are \"offered\" to a user on your paywall. They're an optional, but recommended feature of RevenueCat that can make it easier to change and experiment with your paywalls.</p> <ul> <li>Each offering is going to contain one or more packages.</li> </ul> </li> </ul>"},{"location":"notes/revenue_cat/#revenue-cat-example-illustration","title":"Revenue Cat Example Illustration","text":""},{"location":"projects/dumbo_xiangqi_ai/","title":"Dumbo - A Xiangqi AI Player","text":"<ul> <li>View code and other notes in our repository</li> </ul>"},{"location":"projects/dumbo_xiangqi_ai/#overview","title":"Overview","text":"<p>In SUTD's Artificial Intelligence class we were assigned with the task of implementing an algorithm or concept we learned in class as a project. In a team of 4 we implemented the Mini-Max algorithm for the game Xiangqi. We chose to name our AI Dumbo because Xiangqi also means elephant chess. </p> <p>Xiangqi, also known as Chinese Chess, is a zero sum game that resembles that of Western Chess. Xiangqi is popular in China and other East Asian countries. The game is played on a board divided into ten horizontal lines and nine vertical lines, creating a total of 90 intersections. Each player has 16 pieces, with various abilities. These pieces include the General, Advisors, Elephants, Horses, Chariots, Cannons, and Soldiers. Each piece has its own movement abilities and strategic importance in the game. </p> <p>The game has a Game Tree Complexity of ${10^{150}}$ meaning that there are more numbers of possible games to be played than chess. This illustrates the difficulty a computer may have searching through all the possible states efficiently.</p> <p> Game Tree Complexity State-Space Complexity Chess ${10^{123}}$ ${10^{44}}$ Xiangqi ${10^{150}}$ ${10^{40}}$ <p></p>"},{"location":"projects/dumbo_xiangqi_ai/#implementation","title":"Implementation","text":"<p>It was coded using python with the GUI done in pygame. </p>"},{"location":"projects/dumbo_xiangqi_ai/#minimax-algorithm","title":"MiniMax Algorithm","text":"<p>The Minimax algorithm is a decision-making strategy commonly employed in two-play games with deterministic information. It determines the optimal move for a player by minimizing their maximum possible loss. At each level in the algorithm it evaluates the players position at each possibility based on a predetermined evaluation heuristic. For our project we used value of material, mobility of piece, and threats. </p> <p>Dumbo was able to achieve a search depth of 2 with just the basic implementation with it taking around 30 seconds per move. </p>"},{"location":"projects/dumbo_xiangqi_ai/#alpha-beta-pruning","title":"Alpha-Beta Pruning","text":"<p>The next step was to implement optimizers in the search algorithm. Alpha-Beta pruning is a very common optimizer for this algorithm. It keeps track of the best move for itself and the worst move for its opponent. If it finds an better move for its opponent it knows it can already 'prune' that branch and not keep looking. At best case scenario it could prune thousands of options to look through. </p>"},{"location":"projects/dumbo_xiangqi_ai/#move-ordering","title":"Move Ordering","text":"<p>Move ordering helps Alpha-Beta pruning by putting the most likely to be best moves first so that the best case scenario of Alpha-Beta pruning occurs more often. </p> <p>With the implementation of these two optimizers Dumbo was able to achieve a search depth of 3 with it taking only about 5 seconds per move. </p>"},{"location":"projects/dumbo_xiangqi_ai/#results","title":"ResultsStart MenuIn Game","text":"<p>We also created a playable GUI. The user selects what color they would like to play as and then the AI assumes the opposite color. Dumbo was able to beat myself and 1 other teammate who were novices to the game. Dumbo also won a few games against online intermediate bots. </p>"},{"location":"projects/em_algorithm/","title":"Image Object Detection Using GMM and the Expectation-Maximization Algorithm","text":"<p>This project was done for a class assignment at SUTD for Machine Learning. io_data.py was provided for me as well as the image data in .jpg and .txt format. The rest of the code and implementation was written by me. Start by initializing the env by running <code>python3 -m venv env</code> then run <code>source env/bin/activate</code> then run <code>pip install -r requirements.txt</code>. Then either run as a Jupyter Notebook with the env kernel or run <code>python3 main.py</code>. The output will be the segmented images.</p> <ul> <li>See code in repository</li> </ul>"},{"location":"projects/em_algorithm/#implementation","title":"Implementation","text":"<p>My implementation used the given read_data() function to collect the LAB color values for ach pixel. It is then converted to a numpy array of $n$ by $d$ dimensions. $n$ being the number of pixels in the image and ${d}$ being 3 for each of the LAB values. I then initialized my weights, mean, and covariances with the kmeanInitialization() function. This function used the one iteration of k means to Cluster each of the points. From there the weights were initialized by $\\frac{n_k}{n}$ for each value of ${k}$ where ${k}$ is 2 for the number of objects. In this case background and subject. The mean was initialized by taking the average of the individual ${L}$, ${A}$, and ${B}$ values for each cluster. The covariance is initialized the same way with the variance formula. </p> <p>Once the initialization step was done it went into a loop that checks for convergence. Then the E-step was next. In the E-step the responsibilities were calculated using the weights multiplied by the probability distribution function. This is done column by column where each column is an attribute ${k}$. Then I divided each row by the sum of the rows values to get a soft classification where the classified one will approach 1 and the unclassified one will approach 0.</p> <p>Moving into the M-step, I then needed to update the respective means, covariances, weights and get the log likelihood to try and maximize that. This was done with the formulas in the slides and similar to the way they were initialized in the first place. Once the values were updated, I checked for convergence by comparing the old likelihood with the new one. Once the loop was complete, I had a function that got all of the labels and updated the image data with the according LAB values for the write_data() function. Those values were either their originals if it was the unmasked part or black if it was masked. In the main function, data was collected and stored in jpg format with the read_data() function.</p> <p>Overall, this is a more probabilistic way to approach k-means. By using a probability distribution function and soft assignments it is less harsh than k means would be with its classifications functions. It allows for better generalization. Some updates and changes I had to make along the way was with the k-mean initialization. It was originally initialized with random centroids in the data. However, I made it so the algorithm would get stuck in a local minimum that wasn't a good classification. I decided to hard code initialization values that would lead to the right minimum and the results improved dramatically. The output images are shown in results. </p>"},{"location":"projects/em_algorithm/#results","title":"ResultsInputOutputInputOutputInputOutputInputOutput","text":""},{"location":"projects/em_algorithm/#update","title":"Update","text":"<p>This algorithm reflects similar steps to the Kalman filter. There are some key differences however it uses Gaussian probability distributions to predict then update in a repetitive cycle. See my post about Kalman Filters to learn more.</p>"},{"location":"projects/eternal_flame_simulation/","title":"Eternal Flame","text":"<p>Eternal Flame was a team project for my Computer Graphics class at SUTD. The goal of the project was to simulate particle motion, to model a flame, and render the particles with a build board implementation. This was built in C++ with OpenGL.</p>"},{"location":"projects/eternal_flame_simulation/#particle-motion","title":"Particle Motion","text":"<p>The general motion of the flame was controlled using newtons equations for acceleration and velocity. We then wanted to receive input from the user which allowed them to change wind direction and magnitude and the amount of oxygen. The wind effect was achieved by by translating the particle system along an axis. The acis was user defined and the effect resulted in a shear of the particle system. The oxygen effect was meant to decrease or increase the size of the flame. This was achieved by scaling the velocity and time-to-live variables according the amount of oxygen concentration the user inputs.</p>"},{"location":"projects/eternal_flame_simulation/#billboard-rendering","title":"Billboard Rendering","text":"<p>Rendering particles can be expensive as the number of particles in the system grows. If a flame simulation, that requires many particles, were each rendered as 3D objects the rendering of each particle would quickly get very expensive. To solve this problem we implemented the Billboard method of rendering. Rather than representing a particle as a 3D object, it is represented as a 2D textured plane. This 2d plan is then oriented so that it always faces the camera. The particles now oriented towards the camera position will be ordered from front to back to decide what needs to be rendered. The speed up of this rendering process is significant.</p> <p>We saw the benefits in our project through this implementation. We were able to render 500 particles to achieve a realistic motion and rendering of a candle flame and lighter flame.</p>"},{"location":"projects/eternal_flame_simulation/#shaders-and-textures","title":"Shaders and Textures","text":"<p>We used 2 different shaders when rendering everything in our scene. One was used for the particles and the other for the 3D model of the candle or lighter. These shaders were implemented with Phone lighting. The were then compiled and rendered on the GPU through OpenGL's shared implementation.</p>"},{"location":"projects/eternal_flame_simulation/#results","title":"ResultsLighterCandle","text":""},{"location":"projects/kalman_filter_state_estimate_/","title":"Standard Kalman Filter","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d  \nimport copy\nimport math\n</pre> import numpy as np import random import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import axes3d   import copy import math In\u00a0[2]: Copied! <pre>RAN_ACC = np.array([i for i in np.linspace(-.2, .2, 128)])\n\ndef compute_acceleration(acceleration):\n    return acceleration + np.array([random.choice(RAN_ACC), random.choice(RAN_ACC), random.choice(RAN_ACC)])\n\n\ndef generate_data(initial_position, initial_velocity, initial_acceleration, time_step, num_steps):\n    position = initial_position\n    velocity = initial_velocity\n    acceleration = initial_acceleration\n\n    positions = np.array([])\n    velocities = np.array([])\n    accelerations = np.array([])\n    n = len(initial_position)\n    data = []\n\n    # Initial\n    positions = np.append(positions, position)\n    velocities = np.append(velocities,velocity)\n    accelerations = np.append(accelerations, acceleration)\n\n    for _ in range(1, num_steps, 1):\n\n        position += velocity * time_step\n        velocity += acceleration * time_step\n        acceleration = compute_acceleration(acceleration)\n\n        positions = np.vstack([positions, position])\n        velocities = np.vstack([velocities, velocity])\n        accelerations = np.vstack([accelerations, acceleration])\n\n    return  positions, velocities, accelerations\n</pre>    RAN_ACC = np.array([i for i in np.linspace(-.2, .2, 128)])  def compute_acceleration(acceleration):     return acceleration + np.array([random.choice(RAN_ACC), random.choice(RAN_ACC), random.choice(RAN_ACC)])   def generate_data(initial_position, initial_velocity, initial_acceleration, time_step, num_steps):     position = initial_position     velocity = initial_velocity     acceleration = initial_acceleration      positions = np.array([])     velocities = np.array([])     accelerations = np.array([])     n = len(initial_position)     data = []      # Initial     positions = np.append(positions, position)     velocities = np.append(velocities,velocity)     accelerations = np.append(accelerations, acceleration)      for _ in range(1, num_steps, 1):          position += velocity * time_step         velocity += acceleration * time_step         acceleration = compute_acceleration(acceleration)          positions = np.vstack([positions, position])         velocities = np.vstack([velocities, velocity])         accelerations = np.vstack([accelerations, acceleration])      return  positions, velocities, accelerations  In\u00a0[3]: Copied! <pre>initial_position = np.array([1.0, 1.0, 1.0])\ninitial_velocity = np.array([1.0, 0.5, 0.0])\ninitial_acceleration = np.array([.5, .0, .5])\ntime_step = .2\nnum_steps = 100\npositions, velocities, accelerations = generate_data(initial_position,initial_velocity,initial_acceleration,time_step, num_steps)\n\np = copy.deepcopy(positions)\nv = copy.deepcopy(velocities)\na = copy.deepcopy(accelerations)\n</pre> initial_position = np.array([1.0, 1.0, 1.0]) initial_velocity = np.array([1.0, 0.5, 0.0]) initial_acceleration = np.array([.5, .0, .5]) time_step = .2 num_steps = 100 positions, velocities, accelerations = generate_data(initial_position,initial_velocity,initial_acceleration,time_step, num_steps)  p = copy.deepcopy(positions) v = copy.deepcopy(velocities) a = copy.deepcopy(accelerations) In\u00a0[4]: Copied! <pre># Plotting positional data\n\nx = positions[:,0]\ny = positions[:, 1]\nz = positions[:, 2]\n\nfig = plt.figure()\nactual = fig.add_subplot(111, projection='3d')\nimg = actual.scatter(x, y, z)\nplt.show()\n</pre> # Plotting positional data  x = positions[:,0] y = positions[:, 1] z = positions[:, 2]  fig = plt.figure() actual = fig.add_subplot(111, projection='3d') img = actual.scatter(x, y, z) plt.show() In\u00a0[5]: Copied! <pre>POS_VAR = np.array([i for i in np.linspace(-5, 5, 256)])\nVEL_VAR = np.array([i for i in np.linspace(-.25, .25, 128)])\nACC_VAR = np.array([i for i in np.linspace(-.1, .1, 128)])\n\n# Meant to vary the data to simulate measurement and error\ndef simulateMeasuredData(position, velocity, acceleration):\n\n    # Vary Data\n    n = len(position)\n    for i in range(n):\n        position[i] += random.choice(POS_VAR)\n        velocity[i] += random.choice(VEL_VAR)\n        acceleration[i] += random.choice(ACC_VAR)\n\n    X = np.concatenate([position,velocity], axis=0).T\n\n    # Get P matrix. Just random values to simulate an estimated error in the measurement\n    P_pos = np.array([np.abs(random.choice(POS_VAR)), np.abs(random.choice(POS_VAR)), np.abs(random.choice(POS_VAR))])\n    P_vel = np.array([np.abs(random.choice(VEL_VAR)), np.abs(random.choice(VEL_VAR)), np.abs(random.choice(VEL_VAR))])\n    P_temp = np.concatenate([P_pos,P_vel])\n\n    # No covariances because its an example so for simplicity all variables are independent\n    P = np.identity((6)) \n    for i in range(6):\n        P[i,i] = P_temp[i]\n        \n    return X, P, acceleration\n        \ndef print_mat(mat):\n    for line in mat:\n        print(\"|\", end=\" \")\n        for val in line:\n            print(\"%.2f\" % val,end=\"\\t\")\n        print(\"|\")\n\n\ndef getA(X, delta_t):\n    n = len(X)\n    mat = np.identity(n)\n    for i in range((int)(n/2)):\n        mat[i, i+3] = delta_t\n    return mat\n\n    \ndef getB(mat, delta_t):\n    B = np.zeros((6,3))\n    for i in range(3):\n        B[i, i] = .5 * delta_t**2\n    for i in range(3,6,1):\n        B[i,i-3] = delta_t\n    return B\n</pre> POS_VAR = np.array([i for i in np.linspace(-5, 5, 256)]) VEL_VAR = np.array([i for i in np.linspace(-.25, .25, 128)]) ACC_VAR = np.array([i for i in np.linspace(-.1, .1, 128)])  # Meant to vary the data to simulate measurement and error def simulateMeasuredData(position, velocity, acceleration):      # Vary Data     n = len(position)     for i in range(n):         position[i] += random.choice(POS_VAR)         velocity[i] += random.choice(VEL_VAR)         acceleration[i] += random.choice(ACC_VAR)      X = np.concatenate([position,velocity], axis=0).T      # Get P matrix. Just random values to simulate an estimated error in the measurement     P_pos = np.array([np.abs(random.choice(POS_VAR)), np.abs(random.choice(POS_VAR)), np.abs(random.choice(POS_VAR))])     P_vel = np.array([np.abs(random.choice(VEL_VAR)), np.abs(random.choice(VEL_VAR)), np.abs(random.choice(VEL_VAR))])     P_temp = np.concatenate([P_pos,P_vel])      # No covariances because its an example so for simplicity all variables are independent     P = np.identity((6))      for i in range(6):         P[i,i] = P_temp[i]              return X, P, acceleration          def print_mat(mat):     for line in mat:         print(\"|\", end=\" \")         for val in line:             print(\"%.2f\" % val,end=\"\\t\")         print(\"|\")   def getA(X, delta_t):     n = len(X)     mat = np.identity(n)     for i in range((int)(n/2)):         mat[i, i+3] = delta_t     return mat       def getB(mat, delta_t):     B = np.zeros((6,3))     for i in range(3):         B[i, i] = .5 * delta_t**2     for i in range(3,6,1):         B[i,i-3] = delta_t     return B  In\u00a0[6]: Copied! <pre>X_0, P_0, mu_k = simulateMeasuredData(p[0], v[0], a[0])\n</pre> X_0, P_0, mu_k = simulateMeasuredData(p[0], v[0], a[0]) In\u00a0[7]: Copied! <pre>A = getA(X_0, time_step)\nB = getB(X_0, time_step)\nmu = accelerations[0]\n</pre> A = getA(X_0, time_step) B = getB(X_0, time_step) mu = accelerations[0] In\u00a0[8]: Copied! <pre># New Predicted State\nAX = np.dot(A, X_0)\nBmu = np.dot(B,mu)\n\nX_kp = AX + Bmu # No omega because simplicity\n# No Q value. Q should be some value accounting for error in calculating this prediction\nP_kp = np.dot(np.dot(A,P_0), A.T)\n</pre> # New Predicted State AX = np.dot(A, X_0) Bmu = np.dot(B,mu)  X_kp = AX + Bmu # No omega because simplicity # No Q value. Q should be some value accounting for error in calculating this prediction P_kp = np.dot(np.dot(A,P_0), A.T)   <ul> <li>Visualization of the process for the state matrix</li> </ul> In\u00a0[9]: Copied! <pre>print(\"A * X_0 + B * mu + omega\\n\")\nprint(X_0)\nprint(\"*\")\nprint_mat(A)\nprint(\"+\")\nprint_mat(B)\nprint(\"*\")\nprint(mu)\nprint(\"=\")\nprint(X_kp)\n</pre> print(\"A * X_0 + B * mu + omega\\n\") print(X_0) print(\"*\") print_mat(A) print(\"+\") print_mat(B) print(\"*\") print(mu) print(\"=\") print(X_kp) <pre>A * X_0 + B * mu + omega\n\n[-0.19607843 -3.41176471  2.15686275  0.75393701  0.3523622  -0.17913386]\n*\n| 1.00\t0.00\t0.00\t0.20\t0.00\t0.00\t|\n| 0.00\t1.00\t0.00\t0.00\t0.20\t0.00\t|\n| 0.00\t0.00\t1.00\t0.00\t0.00\t0.20\t|\n| 0.00\t0.00\t0.00\t1.00\t0.00\t0.00\t|\n| 0.00\t0.00\t0.00\t0.00\t1.00\t0.00\t|\n| 0.00\t0.00\t0.00\t0.00\t0.00\t1.00\t|\n+\n| 0.02\t0.00\t0.00\t|\n| 0.00\t0.02\t0.00\t|\n| 0.00\t0.00\t0.02\t|\n| 0.20\t0.00\t0.00\t|\n| 0.00\t0.20\t0.00\t|\n| 0.00\t0.00\t0.20\t|\n*\n[0.5 0.  0.5]\n=\n[-0.03529103 -3.34129226  2.13103597  0.85393701  0.3523622  -0.07913386]\n</pre> <ul> <li>Calculating Kalman Gain and X_k</li> </ul> In\u00a0[10]: Copied! <pre># Kalman Gain should be n X n with n being the size of X\n# H is just a resizing matrix but in this case our P is the right size\n\n# Get New Measurement\nY, R, acceleration = simulateMeasuredData(p[1], v[1], a[1])\n</pre> # Kalman Gain should be n X n with n being the size of X # H is just a resizing matrix but in this case our P is the right size  # Get New Measurement Y, R, acceleration = simulateMeasuredData(p[1], v[1], a[1]) In\u00a0[11]: Copied! <pre>temp1 = np.linalg.inv(P_kp + R)\nK = np.dot(P_kp,temp1)\n\nX_k = X_kp + np.dot(K, (Y - X_kp))\n</pre> temp1 = np.linalg.inv(P_kp + R) K = np.dot(P_kp,temp1)  X_k = X_kp + np.dot(K, (Y - X_kp)) In\u00a0[12]: Copied! <pre>print(X_k)\n</pre> print(X_k) <pre>[ 2.00174805 -2.3256714  -0.08073956  1.0454063   0.54094919 -0.08660836]\n</pre> <ul> <li>Update P_k<ul> <li>This step is very important because it updates how confident we are in our current value X_k. It says how confident we are mid process and whether we should be trusting it.</li> </ul> </li> </ul> In\u00a0[13]: Copied! <pre># Update P_k\nP_k = np.dot((np.identity(K.shape[0]) - K), P_kp)\n\nprint_mat(P_k)\n</pre> # Update P_k P_k = np.dot((np.identity(K.shape[0]) - K), P_kp)  print_mat(P_k) <pre>| 2.32\t0.00\t0.00\t0.00\t0.00\t0.00\t|\n| 0.00\t0.96\t0.00\t0.00\t0.00\t0.00\t|\n| 0.00\t0.00\t1.54\t0.00\t0.00\t0.01\t|\n| 0.00\t0.00\t0.00\t0.04\t0.00\t0.00\t|\n| 0.00\t0.00\t0.00\t0.00\t0.01\t0.00\t|\n| 0.00\t0.00\t0.01\t0.00\t0.00\t0.07\t|\n</pre> In\u00a0[14]: Copied! <pre>Xlist = []\nPlist = []\n\nX_k, P_k, mu_k = simulateMeasuredData(p[0], v[0], a[0])\n\nXlist.append(X_k)\nPlist.append(P_k)\n\nfor i in range(1, num_steps):\n    A = getA(X_k, time_step)\n    B = getB(X_k, time_step)\n\n    # New Predicted State\n    X_kp = np.dot(A, X_k) + np.dot(B,mu_k)\n    P_kp = np.dot(np.dot(A,P_k), A.T) \n\n    # Update w/ Measurement State\n    Y, R, mu_k = simulateMeasuredData(p[i], v[i], a[i])\n    K = np.dot(P_kp,np.linalg.inv(P_kp + R))\n    X_k = X_kp + np.dot(K, (Y - X_kp))\n\n    # Update P_k\n    P_k = np.dot((np.identity(K.shape[0]) - K), P_kp)\n\n    # Save output\n    Xlist.append(X_k)\n    Plist.append(P_k)\n</pre> Xlist = [] Plist = []  X_k, P_k, mu_k = simulateMeasuredData(p[0], v[0], a[0])  Xlist.append(X_k) Plist.append(P_k)  for i in range(1, num_steps):     A = getA(X_k, time_step)     B = getB(X_k, time_step)      # New Predicted State     X_kp = np.dot(A, X_k) + np.dot(B,mu_k)     P_kp = np.dot(np.dot(A,P_k), A.T)       # Update w/ Measurement State     Y, R, mu_k = simulateMeasuredData(p[i], v[i], a[i])     K = np.dot(P_kp,np.linalg.inv(P_kp + R))     X_k = X_kp + np.dot(K, (Y - X_kp))      # Update P_k     P_k = np.dot((np.identity(K.shape[0]) - K), P_kp)      # Save output     Xlist.append(X_k)     Plist.append(P_k)      <ul> <li>Plot of a portion of the data</li> </ul> In\u00a0[15]: Copied! <pre># Convert and graph output\nest_position = np.array([])\nest_position = np.append(est_position, np.array([0.0,0.0,0.0]))\n\nfor item in Xlist:\n    est_position = np.vstack([est_position, np.array([item[0], item[1], item[2]])])\n\nest_position = np.delete(est_position, 0,0)\nest_x = est_position[:, 0]\nest_y = est_position[:, 1]\nest_z = est_position[:, 2]\n\nmeas_x = p[:, 0]\nmeas_y = p[:, 1]\nmeas_z = p[:, 2]\n\nx_axis = (0, 25)\ny_axis = (0, 25)\nz_axis = (0, 25)\nfig = plt.figure()\n\nax = fig.add_subplot(111, projection='3d')\nax.set_xlim(x_axis)\nax.set_ylim(y_axis)\nax.set_zlim(z_axis)\nax.scatter(x, y, z, label=\"Actual\")\nax.scatter(meas_x,meas_y, meas_z, label=\"Measured\")\nax.scatter(est_x, est_y, est_z, label=\"Kalman\")\nax.legend()\nplt.show()\n</pre> # Convert and graph output est_position = np.array([]) est_position = np.append(est_position, np.array([0.0,0.0,0.0]))  for item in Xlist:     est_position = np.vstack([est_position, np.array([item[0], item[1], item[2]])])  est_position = np.delete(est_position, 0,0) est_x = est_position[:, 0] est_y = est_position[:, 1] est_z = est_position[:, 2]  meas_x = p[:, 0] meas_y = p[:, 1] meas_z = p[:, 2]  x_axis = (0, 25) y_axis = (0, 25) z_axis = (0, 25) fig = plt.figure()  ax = fig.add_subplot(111, projection='3d') ax.set_xlim(x_axis) ax.set_ylim(y_axis) ax.set_zlim(z_axis) ax.scatter(x, y, z, label=\"Actual\") ax.scatter(meas_x,meas_y, meas_z, label=\"Measured\") ax.scatter(est_x, est_y, est_z, label=\"Kalman\") ax.legend() plt.show()"},{"location":"projects/kalman_filter_state_estimate_/#standard-kalman-filter","title":"Standard Kalman Filter\u00b6","text":"<p>The goal of the project was to solidify my basic understanding of the step by step process of the Kalman Filter algorithm. This implementation of the Kalman Filter was with an object in 3D space with simulated position, velocity, and acceleration. I then simulated a measuring of these points by randomly scrambling the data within a certain range. I acknowledge that the simulated data was not perfect and by using the random function to generate measurements plays into the most ideal scenario for a Kalman filter because the data will be a perfect normal distribution. This however was to demonstrate the process of a Kalman Filter regardless of the data.</p>"},{"location":"projects/kalman_filter_state_estimate_/#sources","title":"Sources\u00b6","text":"<ul> <li><p>Kalman and Bayesian Filters Online Textbook</p> </li> <li><p>Kalman Filter Youtube Tutorial</p> </li> </ul>"},{"location":"projects/kalman_filter_state_estimate_/#simulating-path-of-a-3d-object","title":"Simulating Path of a 3D object\u00b6","text":""},{"location":"projects/kalman_filter_state_estimate_/#step-by-step-walk-through","title":"Step by Step walk through\u00b6","text":""},{"location":"projects/kalman_filter_state_estimate_/#demonstration-of-one-iteration-of-the-above-process","title":"Demonstration of one iteration of the above process\u00b6","text":"<ul> <li>Predicted State Step</li> </ul>"},{"location":"projects/kalman_filter_state_estimate_/#simulated-kalman-filter-loop","title":"Simulated Kalman Filter Loop\u00b6","text":""},{"location":"projects/kalman_filter_state_estimate_/#analysis-of-results","title":"Analysis of results\u00b6","text":""},{"location":"projects/multiecho_and_feedback_comb_filter/","title":"Python Mini Project","text":"In\u00a0[1]: Copied! <pre>%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nimport scipy.signal as sig\n</pre> %matplotlib inline import numpy as np import matplotlib.pyplot as plt import soundfile as sf import scipy.signal as sig  In\u00a0[2]: Copied! <pre>x, fs = sf.read('../assets/audio_files/data_speech.wav')\nplt.figure(figsize=(8,4))\nt = np.arange(len(x)) / fs\n\nplt.plot(t, x)\nplt.xlabel(r'$t$ in s')\nplt.ylabel(r'$x[k]$')\nplt.axis([0, t[-1], -1, 1]);\n</pre> x, fs = sf.read('../assets/audio_files/data_speech.wav') plt.figure(figsize=(8,4)) t = np.arange(len(x)) / fs  plt.plot(t, x) plt.xlabel(r'$t$ in s') plt.ylabel(r'$x[k]$') plt.axis([0, t[-1], -1, 1]); In\u00a0[3]: Copied! <pre>'''\nTODO:\n\n* Define parameters alpha, R, and N\n    * alpha^k is the magnitude\n    * k*R is the offset\n    * k iterates from 0 to N-1\n\n* for each value in x \n    * propogate its echo N-1 times\n        * for each propogation add it to the sum\n    * add to y vector\n'''\ndef multiEchoFilter(waveform, alpha, R, N):\n  y = []\n\n  pos = 0\n  for x_val in waveform:\n    Sum = 0\n\n    for k in range(N-1):\n      \n      if ((pos-(k*R)) &gt; 0):\n        Sum += (waveform[pos-k*R] * (alpha**k))\n      else:\n        break\n    \n    y.append(Sum + waveform[pos])\n\n    pos+=1\n  return y\n\n\n\n''' \niterate through waveform array\n  For each iteration use formulat alpha*y + x and add it to y\n'''\ndef feedbackCombFilter(waveform, alpha, R):\n  y = []\n  for i in range(len(waveform)):\n    if(i-R &gt; 0):\n      cur = alpha*y[i-R] + waveform[i]\n      y.append(cur)\n    else:\n      y.append(waveform[i])\n\n  return y\n</pre> ''' TODO:  * Define parameters alpha, R, and N     * alpha^k is the magnitude     * k*R is the offset     * k iterates from 0 to N-1  * for each value in x      * propogate its echo N-1 times         * for each propogation add it to the sum     * add to y vector ''' def multiEchoFilter(waveform, alpha, R, N):   y = []    pos = 0   for x_val in waveform:     Sum = 0      for k in range(N-1):              if ((pos-(k*R)) &gt; 0):         Sum += (waveform[pos-k*R] * (alpha**k))       else:         break          y.append(Sum + waveform[pos])      pos+=1   return y    '''  iterate through waveform array   For each iteration use formulat alpha*y + x and add it to y ''' def feedbackCombFilter(waveform, alpha, R):   y = []   for i in range(len(waveform)):     if(i-R &gt; 0):       cur = alpha*y[i-R] + waveform[i]       y.append(cur)     else:       y.append(waveform[i])    return y   In\u00a0[4]: Copied! <pre># Define parameters for multiecho filter\nalpha, R, N = .7, 7777, 5\n\ny_mul_echo = multiEchoFilter(x,alpha, R, N)\n\n# Defining parameters for feedback comb filter\nalpha, R = .7, 6000\n\ny_f_comb = feedbackCombFilter(x,alpha, R)\n</pre> # Define parameters for multiecho filter alpha, R, N = .7, 7777, 5  y_mul_echo = multiEchoFilter(x,alpha, R, N)  # Defining parameters for feedback comb filter alpha, R = .7, 6000  y_f_comb = feedbackCombFilter(x,alpha, R)   In\u00a0[5]: Copied! <pre># Plotting both filters\n\n# Multi Echo \nplt.figure(figsize=(8,4))\nplt.plot(t, y_mul_echo)\nplt.title(\"Multiecho Filter\")\nplt.xlabel(r'$t$ in s')\nplt.ylabel(r'$y[n]$');\n\n# Feedback Comb\nplt.figure(figsize=(8,4))\nplt.plot(t, y_f_comb)\nplt.title(\"Feedback Comb Filter\")\nplt.xlabel(r'$t$ in s')\nplt.ylabel(r'$y[n]$');\n\n\n# Plotting convolution for comparison\nh, fsh = sf.read('../assets/audio_files/data_room_impulse_response.wav')\ny = np.convolve(h, x)\n#normalizing\ny = max(x) * y / np.max(np.abs(y))\nplt.figure(figsize=(8,4))\nt = 1/fs*np.arange(len(y))\nplt.plot(t, y)\nplt.xlabel(r'$t$ in s')\nplt.ylabel(r'$y[k]$');\n</pre> # Plotting both filters  # Multi Echo  plt.figure(figsize=(8,4)) plt.plot(t, y_mul_echo) plt.title(\"Multiecho Filter\") plt.xlabel(r'$t$ in s') plt.ylabel(r'$y[n]$');  # Feedback Comb plt.figure(figsize=(8,4)) plt.plot(t, y_f_comb) plt.title(\"Feedback Comb Filter\") plt.xlabel(r'$t$ in s') plt.ylabel(r'$y[n]$');   # Plotting convolution for comparison h, fsh = sf.read('../assets/audio_files/data_room_impulse_response.wav') y = np.convolve(h, x) #normalizing y = max(x) * y / np.max(np.abs(y)) plt.figure(figsize=(8,4)) t = 1/fs*np.arange(len(y)) plt.plot(t, y) plt.xlabel(r'$t$ in s') plt.ylabel(r'$y[k]$'); In\u00a0[6]: Copied! <pre># Writing two files\nsf.write('../assets/audio_files/multiEchoFilter.wav', y_mul_echo, fs)\nsf.write('../assets/audio_files/FeedbackCombFilter.wav', y_f_comb, fs)\n</pre> # Writing two files sf.write('../assets/audio_files/multiEchoFilter.wav', y_mul_echo, fs) sf.write('../assets/audio_files/FeedbackCombFilter.wav', y_f_comb, fs)"},{"location":"projects/multiecho_and_feedback_comb_filter/#python-mini-project","title":"Python Mini Project\u00b6","text":"<ul> <li>View on my GitHub</li> </ul>"},{"location":"projects/multiecho_and_feedback_comb_filter/#different-types-of-echo-filters","title":"Different types of echo filters\u00b6","text":"<ul> <li>Source: https://github.com/spatialaudio/signals-and-systems-lecture/blob/master/discrete_systems_time_domain/convolution_room_IR.ipynb</li> </ul>"},{"location":"projects/multiecho_and_feedback_comb_filter/#collecting-waveform-data","title":"Collecting waveform data\u00b6","text":"<ul> <li>putting data into x</li> <li>plotting x with respect to time for comparison purposes</li> </ul>"},{"location":"projects/multiecho_and_feedback_comb_filter/#creating-function-for-both-filters","title":"Creating function for both filters\u00b6","text":"<ul> <li><p>First function is for multi echo filter</p> <ul> <li></li> <li>Utilizing varying ranges for alpha R and N to get the right echo</li> </ul> </li> <li><p>Second function is for feedback comb filter</p> <ul> <li></li> <li>Utilizes R and alpha to get a diminishing echo for the response</li> </ul> </li> </ul>"},{"location":"projects/multiecho_and_feedback_comb_filter/#inputing-parameters","title":"Inputing parameters\u00b6","text":"<ul> <li><p>Multiecho Filter</p> <ul> <li>Parameters where chosen as</li> <li>alpha = .7<ul> <li>Through experimenting with different values I discovered that the Alpha value changes how soon the echo diminishes. With an alpha value &lt; .5 there were not enough strong echos for it to sound the the given wet_source.wav. Upon further auditory testing I decided .7 was the closest to it.</li> </ul> </li> <li>R = 7777<ul> <li>The R value changes how far apart the echos are. This one was hard to determine because in the x array each data point accounts for about .0001 of a second. This meant that the R value needed to be high so that the echos carried out accross multiple seconds. Anything under 5000 I determined was too quick and did not sound right. The actual value of 7777 was slightly arbitrary. I just determined that the 7-8 thounsand range sounded the best and looked the most similar graphically.</li> </ul> </li> <li>N = 5<ul> <li>N increases the runtime of the function dramatically. Anything over 15 it starts to take over 20 seconds to run, mostly because of the size of the array. The only noticable change in make is the clarity of the echo. To high and it is not clear, too low and it is too clear.</li> </ul> </li> </ul> </li> <li><p>Feedback Comb filter</p> <ul> <li>alpha = .7 again<ul> <li>Alpha in this equation also does something similar by determining how quickly the echo fades. Upon auditory and graphically testing, it was determined to be .7 again.</li> </ul> </li> <li>R = 6000<ul> <li>R value increases the noisiness of the echo. Anything over 6000 was determined to be too cluttered and noisy.</li> </ul> </li> </ul> </li> </ul>"},{"location":"projects/multiecho_and_feedback_comb_filter/#computing-the-convolution","title":"Computing the convolution\u00b6","text":""},{"location":"projects/multiecho_and_feedback_comb_filter/#writing-to-output-files","title":"Writing to output files\u00b6","text":""},{"location":"projects/twitter_sentiment_analysis/","title":"Twitter Sentiment Analysis Using Viterbi Algorithm","text":"In\u00a0[1]: Copied! <pre># Excerpt of a function written to find emission parameters\n\"\"\"\nEmission parameter estimation using MLE\nInput:\n    * tags: a dictionary of tags, that provides a count for how often a word has occurred for a given tag\n    * k: smoothing parameter for the #UNK# token\n\nReturns:\n    * eParams: a dictionary of tags that calculates a ratio between a tags words and its total count i.e: {\"O\":{\"hi\":0.5,\"#UNK#\":0.5}}\n\"\"\"\n\ndef findEmissionParams(tags, k=1):\n\n    eParams = defaultdict(lambda: defaultdict(lambda: 1e-10))\n    for tag, tagData in tags.items():                                   # for each word under a tag\n        for word, wordCount in tagData['words'].items():                # create the parameter emission = wordCount / totalCount + k\n            eParams[tag][word] = wordCount / (tagData['count'] + k)\n        eParams[tag]['#UNK#'] = k / (tagData['count'] + k)              # update #UNK# token for each tag\n\n    return eParams\n</pre> # Excerpt of a function written to find emission parameters \"\"\" Emission parameter estimation using MLE Input:     * tags: a dictionary of tags, that provides a count for how often a word has occurred for a given tag     * k: smoothing parameter for the #UNK# token  Returns:     * eParams: a dictionary of tags that calculates a ratio between a tags words and its total count i.e: {\"O\":{\"hi\":0.5,\"#UNK#\":0.5}} \"\"\"  def findEmissionParams(tags, k=1):      eParams = defaultdict(lambda: defaultdict(lambda: 1e-10))     for tag, tagData in tags.items():                                   # for each word under a tag         for word, wordCount in tagData['words'].items():                # create the parameter emission = wordCount / totalCount + k             eParams[tag][word] = wordCount / (tagData['count'] + k)         eParams[tag]['#UNK#'] = k / (tagData['count'] + k)              # update #UNK# token for each tag      return eParams In\u00a0[2]: Copied! <pre>def viterbi(observed_data, trans_params, emi_params, words):\n    # Define n = len(observed_data)\n    # Viterbi matrix is n+1 x num_states\n    # backpointer table is n x\n    # This is used to trace back and find the maximum path??\n    # Calculate initial state probabilites using transmission matrix and emission matrix\n    # Fill in the viterbi table\n    # For every word (t) in observed_data\n    # Iterate through the transmission matrix and emission matrix using a double for loop\n    # for every state in states\n    # Probabilities = np.zeros()\n    # for every prev_state in states\n    # probs[s_prev] = viterbi_table[t-1][s_prev] * transmission_matrix[s_prev][s] * emission_matrix[s][observed_data[t]]\n    # viterbi_table[t][s] = np.max(probs)\n    # backpointer_table[t][s] = np.argmax(probs)\n\n    # calculate the probability of the most likely sequence\n\n    # use back tracking to find the sequence of states\n    labels = list(emi_params.keys())\n    n = len(observed_data)\n    num_labels = len(labels)\n\n    # Viterbi Table is 2d dict that contains n x num_states entries. Is used to store the probability paths\n    delta = [{state: 0.0 for state in labels} for word in observed_data]\n    # Back tracing table is a 2d dict that contains the previous key associated with the probability in v_table\n    psi = [{state: None for state in labels} for word in observed_data]\n\n    # Initializing the beginning of the algorithm\n    for l in labels:\n        if observed_data[0] not in words:\n            delta[0][l] = np.log(trans_params['START'][l]) + np.log(emi_params[l]['#UNK#'])\n        else:\n            delta[0][l] = np.log(trans_params['START'][l]) + np.log(emi_params[l][observed_data[0]])\n        psi[0][l] = 'START'\n\n    # Iterating through the rest of the table\n    for t in range(1, n):\n\n        for s in range(num_labels):\n            probability_lst = []\n            for prev_s in range(num_labels):\n                # NOTE: Need to check if the location in trans_params is exists and if the location in emi_params exists\n                #       This is done with .get()\n                if observed_data[t] not in words:\n                    probability_lst.append(float(\n                        delta[t - 1][labels[prev_s]] + np.log(trans_params[labels[prev_s]][\n                            labels[s]]) + np.log(emi_params[labels[s]]['#UNK#'])))\n                else:\n                    probability_lst.append(float(\n                        delta[t - 1][labels[prev_s]] + np.log(trans_params[labels[prev_s]][\n                            labels[s]]) + np.log(emi_params[labels[s]][observed_data[t]])))\n\n            # max of all the probabilities \n            delta[t][labels[s]] = max(probability_lst)\n            psi[t][labels[s]] = labels[np.argmax(probability_lst)]\n\n    # Find the transition from last word to STOP\n    probability_lst = []\n    for prev_s in range(num_labels):\n        probability_lst.append(\n            float(delta[n - 1][labels[prev_s]] + np.log(trans_params[labels[prev_s]]['STOP'])))\n\n    # v_table[n - 1][states[num_states - 1]] = max(probability_lst)\n    # bt_table[n - 1][states[num_states - 1]] = states[np.argmax(probability_lst)]\n\n    # Now time for some back tracing to find the correct sequence with bt_table\n    sequence = [None for i in range(n)]\n\n    # sequence[n - 1] = max(v_table[n - 1], key=lambda k: v_table[n - 1][k])\n    sequence[n-1] = labels[np.argmax(probability_lst)]\n    for t in range(n - 2, -1, -1):\n        sequence[t] = psi[t + 1][sequence[t + 1]]\n    return sequence\n</pre> def viterbi(observed_data, trans_params, emi_params, words):     # Define n = len(observed_data)     # Viterbi matrix is n+1 x num_states     # backpointer table is n x     # This is used to trace back and find the maximum path??     # Calculate initial state probabilites using transmission matrix and emission matrix     # Fill in the viterbi table     # For every word (t) in observed_data     # Iterate through the transmission matrix and emission matrix using a double for loop     # for every state in states     # Probabilities = np.zeros()     # for every prev_state in states     # probs[s_prev] = viterbi_table[t-1][s_prev] * transmission_matrix[s_prev][s] * emission_matrix[s][observed_data[t]]     # viterbi_table[t][s] = np.max(probs)     # backpointer_table[t][s] = np.argmax(probs)      # calculate the probability of the most likely sequence      # use back tracking to find the sequence of states     labels = list(emi_params.keys())     n = len(observed_data)     num_labels = len(labels)      # Viterbi Table is 2d dict that contains n x num_states entries. Is used to store the probability paths     delta = [{state: 0.0 for state in labels} for word in observed_data]     # Back tracing table is a 2d dict that contains the previous key associated with the probability in v_table     psi = [{state: None for state in labels} for word in observed_data]      # Initializing the beginning of the algorithm     for l in labels:         if observed_data[0] not in words:             delta[0][l] = np.log(trans_params['START'][l]) + np.log(emi_params[l]['#UNK#'])         else:             delta[0][l] = np.log(trans_params['START'][l]) + np.log(emi_params[l][observed_data[0]])         psi[0][l] = 'START'      # Iterating through the rest of the table     for t in range(1, n):          for s in range(num_labels):             probability_lst = []             for prev_s in range(num_labels):                 # NOTE: Need to check if the location in trans_params is exists and if the location in emi_params exists                 #       This is done with .get()                 if observed_data[t] not in words:                     probability_lst.append(float(                         delta[t - 1][labels[prev_s]] + np.log(trans_params[labels[prev_s]][                             labels[s]]) + np.log(emi_params[labels[s]]['#UNK#'])))                 else:                     probability_lst.append(float(                         delta[t - 1][labels[prev_s]] + np.log(trans_params[labels[prev_s]][                             labels[s]]) + np.log(emi_params[labels[s]][observed_data[t]])))              # max of all the probabilities              delta[t][labels[s]] = max(probability_lst)             psi[t][labels[s]] = labels[np.argmax(probability_lst)]      # Find the transition from last word to STOP     probability_lst = []     for prev_s in range(num_labels):         probability_lst.append(             float(delta[n - 1][labels[prev_s]] + np.log(trans_params[labels[prev_s]]['STOP'])))      # v_table[n - 1][states[num_states - 1]] = max(probability_lst)     # bt_table[n - 1][states[num_states - 1]] = states[np.argmax(probability_lst)]      # Now time for some back tracing to find the correct sequence with bt_table     sequence = [None for i in range(n)]      # sequence[n - 1] = max(v_table[n - 1], key=lambda k: v_table[n - 1][k])     sequence[n-1] = labels[np.argmax(probability_lst)]     for t in range(n - 2, -1, -1):         sequence[t] = psi[t + 1][sequence[t + 1]]     return sequence In\u00a0[3]: Copied! <pre>\"\"\"\nJelinek Mercer Smoothing\n\"\"\"\ndef findEmissionParamsJelinekMercer(eParams, lambdaVal):\n    smoothedParams = defaultdict(lambda: defaultdict(int))\n    # Compute the total count of all words in the corpus\n    totalWords = sum([sum(tagData.values()) for tagData in eParams.values()])\n    # Calculate the Jelinek-Mercer adjusted probabilities\n    for tag, tagData in eParams.items():\n        for word, count in tagData.items():\n            # Compute the background probability of the word\n            bgProb = sum(eParams[tag2].get(word, 0) for tag2 in eParams.keys()) / totalWords\n            # Calculate the Jelinek-Mercer adjusted probability\n            smoothedParams[tag][word] = (1 - lambdaVal) * (count / sum(tagData.values())) + lambdaVal * bgProb\n\n    return smoothedParams\n</pre> \"\"\" Jelinek Mercer Smoothing \"\"\" def findEmissionParamsJelinekMercer(eParams, lambdaVal):     smoothedParams = defaultdict(lambda: defaultdict(int))     # Compute the total count of all words in the corpus     totalWords = sum([sum(tagData.values()) for tagData in eParams.values()])     # Calculate the Jelinek-Mercer adjusted probabilities     for tag, tagData in eParams.items():         for word, count in tagData.items():             # Compute the background probability of the word             bgProb = sum(eParams[tag2].get(word, 0) for tag2 in eParams.keys()) / totalWords             # Calculate the Jelinek-Mercer adjusted probability             smoothedParams[tag][word] = (1 - lambdaVal) * (count / sum(tagData.values())) + lambdaVal * bgProb      return smoothedParams"},{"location":"projects/twitter_sentiment_analysis/#twitter-sentiment-analysis-using-viterbi-algorithm","title":"Twitter Sentiment Analysis Using Viterbi Algorithm\u00b6","text":"<ul> <li>Note this code is not executable. Code is excerpts from the project which was built in conjunction with two other classmates.</li> </ul>"},{"location":"projects/twitter_sentiment_analysis/#overview","title":"Overview\u00b6","text":"<p>In the SUTD Machine Learning class we were assigned the task of analyzing sentiment of tweets in two different languages using the Viterbi Algorithm. The data set consisted of about 200 tweets from each language. In the training data set each word came with a corresponding tag indicating the sentiment of the word. The only packages we were allowed to use math packages such as <code>math</code> and <code>numpy</code>. Below are excerpts of explanations from our report on the project.</p> <p>The process of \"training\" and \"predicting\" involves finding probabilities between words and the possible tags, as well as the probabilities of one tag coming after another. Then with these probabilities it is then decoded on a test data set using the Viterbi algorithm. This process is discussed more in depth below as well as some ways we chose to improve the accuracy of this algorithm.</p>"},{"location":"projects/twitter_sentiment_analysis/#emission-and-transmission-parameters","title":"Emission and Transmission Parameters\u00b6","text":"<p>Emission parameters are the probabilities of observing a certain word given the sentiment label. This parameter involves going through all of the training data and counting the amount of times a word emits a tag divided by the number of times the word occurs. See the formula below.</p> <p>$$ e(x|y)=   \\left\\{ \\begin{array}{ll}       \\frac{Count(y\\rightarrow x)}{Count(y) + k} &amp; \\text{If the word token appears in the training set} \\\\       \\frac{k}{Count + k} &amp; \\text{If word token x is the special token UNK}  \\\\ \\end{array}  \\right.   $$</p> <p>Transmission parameters is the probabilities of transitions from one sentiment label to another. This probability is calculated using the formula below.</p> <p>$$q(y_i | y_{i-1}) - \\frac{Count(y_{i-1},y_i)}{Count(y_{i-1})}$$</p>"},{"location":"projects/twitter_sentiment_analysis/#the-viterbi-algorithm","title":"The Viterbi Algorithm\u00b6","text":"<p>The key definitions in the algorithm are hidden states and observations. With the emission parameters we calculated the probability that a given observation outputs a hidden state. The transmission parameters contain the probabilities that hidden state ${x}$ comes after hidden state ${y}$. In our case the observation is a word in a tweet and the hidden state is the sentiment label associated with it. The Viterbi Algorithm is a dynamic programming model that leverages these to probabilities to find the most likely sequence of hidden states or tags based off a given observation or tweet. In depth analysis of the algorithm is not discussed here but a function from the project is provided to show our implementation.</p>"},{"location":"projects/twitter_sentiment_analysis/#improvements","title":"Improvements\u00b6","text":""},{"location":"projects/twitter_sentiment_analysis/#second-order-viterbi-algorithm","title":"Second Order Viterbi Algorithm\u00b6","text":"<p>In addition to implementing first order Viterbi, we were also tasked with implementing second order Viterbi. The difference is in the transmission parameters and the decoding of such. The transmission parameters turn into a 3 dimensional matrix that has all the possible combinations of of tags and then populating those combinations with probabilities that they occur. In the decoding stages of the algorithm a whole new dimension is added to keep track of the best path of the previous two states. This means that the immediate previous state best path may not be the best when you look back two steps.</p>"},{"location":"projects/twitter_sentiment_analysis/#smoothing","title":"Smoothing\u00b6","text":"<p>Various smoothing methods were tested on our data. Laplace Smoothing, Good Turing Smoothing, and Jelinek Mercer Smoothing were all tested on the emission parameters of the data. The best smoother for our data will be discussed later. Each of these smothers is a different way to smooth frequency statistics and remove noise that may be within the data. Example is provided below.</p>"},{"location":"projects/twitter_sentiment_analysis/#stop-word-removal","title":"Stop Word Removal\u00b6","text":"<p>Stop word removal is a method that operates under the belief that commonly used words such as 'the', 'a', 'this', 'that' and many others are considered noise in the data. Upon looking at our data set we realized that twitter handles are all unique and always emit the 'O' tag which stands for 'out' or irrelevant to sentiment. This caused a lot of noise in the UNK emission parameter because all tags are unique. So we decided to remove them from the training data and in testing automatically classify them as 'O'. These parameters were then no included in the Viterbi decoding process. Similar things wer don to punctuation and other twitter specific text. Therefore our stop word removal list was custom and not implemented on the standard word list.</p>"},{"location":"projects/twitter_sentiment_analysis/#lemmatization","title":"Lemmatization\u00b6","text":"<p>Lemmatization is another method that can help improve emission parameters in some cases. It is the process of reducing all verbs to their base form. For example 'ran' and 'running' would all become 'run'. It also gets rid of pluralization of words, so 'words' would become just 'word'. This is another form of noise reduction in data. This was implemented with a preexisting lemmatizer in the spaCy python library. It already has one implemented for English and French.</p>"},{"location":"projects/twitter_sentiment_analysis/#results","title":"Results\u00b6","text":"<p>The final results were fine tuned to get the best output. For English the best results came from using Lemmatization, Stop word removal, Jelinek Mercer smoothing with k = 0.1, and first order Viterbi. For French the best results came from using Lemmatization, Stop word removal, Jelinek Mercer smoothing with k = 0.01, and second order Viterbi. With just a small data set we were able to achieve a precision of 71% in English and 64% in French.</p> <p></p>"}]}